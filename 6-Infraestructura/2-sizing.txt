SIZING
======

==> Identificar cu√°ntos nodos se necesitan para cada rol y qu√© 
    caracter√≠sticas hardware deben de tener.

Recordar que nos hicimos dos preguntas:

1.- ¬øCuantos nodos 'Gateway, ' Master', 'Metadata' y 'Slave' necesitamos para nuestro Cluster?
2.- ¬øCuales seran las caracteristicas Hardware de cada nodo seg√∫n su rol?

La primera pregunta es la cantidad de nodos por servidor. ¬øCu√°l es la buena noticia? ya hay un est√°ndar 
para los nodos Gateway, Master y Metadata, pero lamentablemente no existe un est√°ndar para los nodos 
esclavos, porque obviamente dependiendo de que la Empresa puede que necesites m√°s potencia, como puedes 
que necesites menos potencia computacional.

La otra pregunta es acerca de la cantidad de recursos computacionales de cada uno de los servidores. 
B√°sicamente tenemos que responder RAM, CPU y DISCO DURO (para nodos Gateway, Master, Metadata y Slave). 
La buena noticia es que tambi√©n ya existe un est√°ndar el cu√°l va a ser tu punto de partida para poder hacer 
tu sizing. Por lo tanto, ya tenemos resuelto la gran mayor√≠a de del sizing de nuestro Cluster. El √∫nico 
trabajo que vamos a tener que pensar a detalle es cu√°ntos nodos necesita el servidor Slave. 

Antes de revisar cu√°les son los est√°ndares que mencionamos, vamos a hablar de algunas consideraciones 
hardware que tenemos que conocer para poder entender estos est√°ndares. 

---------------------------------------------------------------------------------------------------------------- 
 ______________________________________
|                                      |
|   Consideraciones hardware           |
|   de los recursos computacionales    |
|______________________________________|

¬øQu√© entendemos por Recurso computacional?
------------------------------------------

Es todo aquel elemento de un servidor que te va a permitir transportar datos, almacenarlos y procesarlos. 
Ese es un recurso computacional en un server. 

¬øQu√© tenemos para poder hacer todo eso en un servidor?:

Para transportar los datos: tenemos la interfaz de red. Por ejemplo, tenemos un servidor 1 del cl√∫ster y 
--------------------------- un servidor 2 del cl√∫ster y queremos mover datos del servidor 1 al servidor 2. 
                            Desde el punto de vista de infraestructura, eso significa que se va a mover por 
                            medio de un cable UTP.

El almacenamiento: el almacenamiento puede ser de 2 tipos, "memoria secundaria" y "memoria principal". 
------------------ La memoria secundaria es el disco duro (el almacenamiento permanente) y la memoria 
                   principal es la memoria en donde se realizan los procesamientos (la RAM). Ya sabemos que 
                   los archivos viven en el disco duro, pero al momento de procesarlos se suben en la memoria 
                   RAM. Disco duro es memoria secundaria y memoria RAM es la memoria principal, ah√≠ es donde 
                   t√∫ puedes almacenar los datos 

Procesar: y finalmente hay que procesar la informaci√≥n. Todos los scripts que nosotros implementemos, van a 
--------- ejecutarse en las CPU. Ahora, ¬øqu√© tipos de CPU existen? Existen las CPU gen√©ricas, las que tenemos 
          todos en nuestras laptops. Estas te permiten hacer c√°lculos gen√©ricos. Existen CPUs especializadas, 
          por ejemplo la GPU es una CPU especializada para hacer c√°lculos matriciales. Para ponerlo en t√©rminos 
          simples, se comenta que la GPU va a reemplazar a la CPU porque es m√°s r√°pida, error la GPU solamente 
          es m√°s r√°pida que la CPU cuando implementes algoritmos que hagan c√°lculos matriciales, por ejemplo, 
          los del tipo de DEEP LEARNING. Pero si tu algoritmo no hace c√°lculos matriciales, no tiene sentido 
          que ejecutes sobre una GPU, ah√≠ te va a convenir una CPU. La GPU es mucho m√°s cara que la CPU, as√≠ 
          que hay que saber cu√°ndo optimizar su uso para no estar ejecutando todo en GPU. De hecho, actualmente 
          existe un nuevo tipo de CPU llamada NPU,  que es una CPU orientada a procesamiento del lenguaje 
          natural, es otra especializaci√≥n. Obviamente la NPU no va a reemplazar a la CPU ni a la GPU, cada 
          tipo de CPU cumple objetivos diferentes, eso t√©nganlo en mente. Para el est√°ndar de Big Data, en la 
          parte de procesamiento de datos existen est√°ndares para interfaces de red, para disco duro, para 
          memoria RAM y para CPU, pero actualmente no existe un est√°ndar que tan potente tiene que ser la GPU 
          o la NPU dentro de nuestro Cluster, eso a√∫n no existe. De hecho, uno de los retos actuales que hay en 
          el mundo del Big data es integrar los algoritmos de Deep learning para que tengan todas las ventajas 
          de Big data, se puede hacer Deep Learning, lo dif√≠cil es hacer Deep Learning en un framework escalable 
          sobre Big data.                    


¬øComo estan relacionados todos estos recursos computacionales?
--------------------------------------------------------------

Los datos que queremos procesar entran por la interfaz de red, por un cable UTP casi siempre. Si quieres 
almacenarlos de manera permanente, van al disco duro. Luego al momento de que un script los procese, primero 
se cargan a la memoria RAM, ya sabemos que hay que tunear todo esto y luego tu proceso se ejecuta en la CPU 
y va tomando los registros que hay en memoria RAM para procesar. As√≠ es como funciona el End to End cl√°sico. 
En un Cluster de Big data funciona de la misma manera, solamente que ya sabemos que es un conjunto de 
servidores, pero en esencia todos los servidores hacen esto. Nosotros solamente indicamos el nivel de 
paralizaci√≥n y ya ver√° el propio cl√∫ster en qu√© servidores ejecuta tu programa y en cada servidor se realiza 
esta ejecuci√≥n. 

Cl√°sicamente en entornos de Big Data el est√°ndar de servidores es tener 100 TB de disco duro, 256 GB de RAM y 
40 n√∫cleos de CPU por servidor. Obviamente este est√°ndar va a variar dependiendo de si el Server es Gateway, 
Master, Metadata o un Slave, eso lo vamos a ver en unos momentos.


¬øQu√© consideraciones tenemos que tener en cuenta para cada uno de estos recursos computacionales? 
-------------------------------------------------------------------------------------------------

‚¨§ Respecto a la CPU: 

Mientras m√°s CPU tengamos m√°s procesos en paralelo vamos a poder ejecutar. ¬øC√≥mo se entiende esto? por ejemplo, 
tenemos cuatro CPUs y tenemos cuatro procesos, si nuestro servidor tiene cuatro CPUs podemos ejecutar en paralelo 
cuatro procesos, una en cada CPU. Pero si nuestro servidor tan solo tuviese una CPU y nosotros tuvi√©semos cuatro 
procesos, todos estar√≠an ocupando el tiempo de esta CPU. ¬øQu√© es lo que har√≠a la CPU para procesar cuatro procesos 
de paralelo? tendr√≠a que intercalar las instrucciones de cada uno de estos procesos y ah√≠ ya perdemos la paralelizaci√≥n. 
As√≠ que mientras m√°s CPU tenga, m√°s procesos en paralelo puedes ejecutar. 

Tambi√©n, mientras m√°s CPUs tengamos m√°s paralelizable es nuestro proceso. ¬øEsto que significa? recordar que para 
paralelizar un proceso levantamos containers y cada container tienes CPUs. Si duplicamos el n√∫mero de containers, lo 
que estamos haciendo por debajo es duplicar el n√∫mero de CPUs, as√≠ que mientras m√°s CPUs tengas en tu cl√∫ster, el nivel 
de paralelizaci√≥n de nuestro proceso va a poder crecer m√°s. 


‚¨§ Respecto a la RAM: 

Mientras m√°s RAM tengamos en el cl√∫ster, vamos a poder procesar archivos y tablas m√°s pesadas. Recordemos que lo que 
queremos procesar vive en Disco duro y para hacer el procesamiento hay que cargarlo en la memoria RAM. Si tenemos 16 GB 
de RAM y tenemos un archivo que pesa 100 GB, eso no va a entrar aqu√≠. As√≠ que mientras m√°s grande sea la memoria RAM del 
Cluster, cada vez vas a poder procesar archivos m√°s grandes. 

Mientras m√°s RAM tenga el Cluster, menos memoria virtual vamos a necesitar. ¬øQu√© es esto de la memoria virtual? Digamos, 
por ejemplo, tenemos 10 GB de RAM en el servidor y queremos procesar un archivo de 25 GB. ¬øQu√© es lo que va a pasar? vamos 
a poder cargar 10 GB del archivo que queremos procesar y van a haber 15 GB del archivo que no vamos a poder procesar. 
¬øQu√© es lo que se puede hacer para evitar este problema? es un poquito m√°s complejo, pero para entenderlo al menos en este 
momento, lo que hacen los sistemas operativos es de Disco duro emular memoria RAM. As√≠ que los 15 GB faltantes que no se 
pudieron cargar en esta memoria RAM, se van a cargar aqu√≠ en el Disco duro y esta partici√≥n del Disco duro se le conoce como 
la partici√≥n SWAP. Realmente es un intercambio de paginaci√≥n, pero para no entrar en tantos detalles t√©cnicos, b√°sicamente 
emula memoria RAM desde Disco duro, para que podamos procesar. Pero ya sabemos que el Disco duro es 100 veces m√°s lento que 
la memoria RAM, as√≠ que cuando la CPU pregunte por estos registros tu algoritmo va a ir 100 m√°s lento. As√≠ que mientras m√°s 
RAM tenga tu Cluster menos vas a depender de esta memoria virtual. 


‚¨§ Respecto al Disco Duro:

Mientras m√°s capacidad de almacenamiento tengan nuestros Discos duros, vamos a poder guardar m√°s archivos.

Mientras m√°s Discos duros tengamos en nuestro Cluster, m√°s paralelizable va a ser nuestro proceso de escritura y lectura. 
Recordar que se coment√≥ que una cosa es tener un Disco duro de 5 TB de infraestructura para cada servidor y otra cosa 
completamente diferente es tener 5 Discos duros de 1 TB. Cuando en HDFS le decimos: ‚Äú‚Ä¶oye escribe en este archivo, al utilizar 
un solo Disco duro se va a escribir de manera lineal, porque solo tenemos un cabezal de lectura y escritura, pero si tuvieramos 
una configuraci√≥n de infraestructura de 5 Discos duros, HDFS se da cuenta, parte el archivo en 5 y lo escribe en 5 Discos duros 
diferentes de manera paralela y lo va a escribir m√°s r√°pido. Eso es otro aspecto de infraestructura, no solamente es la capacidad 
de Disco, sino, tambi√©n el n√∫mero de Discos. Siempre se va a preferir m√°s Discos duros. 


‚¨§ Respecto al Ancho de banda:

Mientras m√°s ancho de banda tenga nuestra infraestructura, la transferencia de archivos entre nodos, va a ser m√°s r√°pida.



--------------------------
CPU
--------------------------


CPU vs uCPU vs vCPU 
-------------------

Realmente a nivel de Big Data no se procesa nada en la CPU. Hay otros dos conceptos que tenemos que conocer:  


ü°Ü N√∫cleos de CPU (uCPU)

Si decimos, tenemos un servidor que tiene 4 CPUs, ¬øcu√°ntos procesos podemos ejecutar en paralelo? Uno podr√≠a pensar, 4 procesos, 
1 en cada CPU. No, a√∫n nos falta informaci√≥n. La CPU dentro tienen n√∫cleos de CPU, realmente quien procesa no es la CPU, son sus 
n√∫cleos, por ejemplo, quiz√°s hemos escuchado, no s√© pues, es una laptop de 4 CPUs de 2 n√∫cleos cada uno. Entonces si tenemos un 
Server de 4 CPUs, donde cada CPU tiene 2 n√∫cleos, entonces, no son 4 procesos, son 8 procesos en paralelo que podemos ejecutar. 
Cada n√∫cleo de CPU es quien procesa. Por ejemplo: 

                                        Cada CPU del servidor tiene 4 n√∫cleos (uCPU)
                                        CPU totales: 6 CPUs
                                        N√∫cleos de CPU totales: 6 x 4 = 24 uCPUs

                                        N√∫cleos f√≠scos de CPU =====>  24 uCPUs                                        

Para este ejemplo si tenemos 6 CPUs f√≠sicas, donde cada CPU tiene 4 n√∫cleos, eso significa que tenemos 24 n√∫cleos de CPU (uCPU) 
en nuestro servidor y por lo tanto podemos ejecutar en ese Server hasta 24 procesos en paralelo. 


ü°Ü Virtual CPU (vCPU)

                                        Factor de Hyperthreading: 1.5
                                        Virtual CPU totales: 24 uCPUs x 1.5 = 36 vCPUs

                                        N√∫cleos virtuales de CPU =====> 36 vCPUs

Antes de explicar lo que es una virtual CPU, vamos a ver c√≥mo es esto de su c√°lculo. Hay unas tecnolog√≠as, por ejemplo, llamadas 
hyperthreading, bueno esta es de propiedad de Intel, pero en general, dependiendo de la tecnolog√≠a que se utilice para construir 
las CPUs, hay un factor que tenemos que conocer. ¬øPara qu√© utilizamos este factor de hyperthreading? 24 uCPUs son los n√∫cleos 
f√≠sicos que tenemos,  pero en nuestro servidor tenemos una tecnolog√≠a de hyperthreading, por lo tanto, ¬øcu√°l es el factor de 
hyperthreading? 1.5. ¬øcu√°l es el n√∫mero de n√∫cleos de CPU? 24, que por el factor de hyperthreading nos da 36. ¬øQu√© significa esto? 
Que con 24 n√∫cleos de CPU podemos ejecutar hasta 36 procesos en paralelo. A este numerito que obtenemos le llevamos vCPUs, porque 
son CPUs virtuales. F√≠sicamente hay 24 CPUs, pero virtualmente, como podemos ejecutar 36 procesos en paralelo, es como si hubiese 
36 n√∫cleos de CPU, pero como no son f√≠sicos y son virtuales, se les llama vCPUs. 

As√≠ que otra pregunta en tu infraestructura que tendr√≠a que hacer es no solamente ¬øcu√°ntas CPUs tenemos y cu√°ntos n√∫cleos tiene 
cada CPU?, si no, si ¬øesas CPUs tienen alg√∫n factor de hyperthreading? la respuesta puede ser, no, entonces, ¬øcu√°ntas vCPUs tienes? 
las mismas que n√∫cleos de CPUs, 24, porque no hay factor de hyperthreading. La respuesta puede ser, s√≠ tenemos un factor de 
hyperthreading ,perfecto, la siguiente pregunta es y, ¬øcu√°l es ese factor? El encargado de infraestructura puede que no tenga idea 
de cu√°l sea el factor. Si no sabe cu√°l es el factor, asume que es 1.5 y, por lo tanto, tendr√≠as 36 vCPUs. Eso significa que cada 
servidor puede correr hasta 36 procesos en paralelo. Por otro lado, si tienes suerte el encargado de infraestructura si conoce el 
valor del factor y nos va a dar un valor que oscila entre 1.3 y 1.7. Nos dir√° el valor y lo multiplicamos por el numero de nucleos 
f√≠sicos de CPUs y obtendremos el valor de vCPUs, que representan cu√°ntos procesos en paralelo puede correr un servidor. 


vCPU : Escalabilidad lineal
---------------------------

¬øPor qu√© es importante saber cu√°ntas vCPUs tenemos a disposici√≥n nuestra? porque si nuestros procesos son autom√°ticamente 
paralelizables, digamos que el proceso 1, con una vCPU, se toma 24 horas. Entonces ese mismo proceso con 2 vCPU se va a tomar la 
mitad del tiempo y con 3 veces vCPU se va a tomar la tercera parte de ese tiempo. Ahora, obviamente tenemos un l√≠mite de vCPUs. 
Tenemos nuestro cluster y digamos que tenemos 40 vCPUs en cada servidor, si tenemos 40 vCPUs y 10 servidores, tendremos 400 vCPUs 
para poder hacer lo que nosotros queramos. Ahora si un proceso dice ‚Äú‚Ä¶oye mira mi nivel de paralelizaci√≥n quiero que sea de 800, 
voy a ponerme 800 vCPUs‚Äù. Eso no tiene ning√∫n sentido porque en infraestructura solamente tenemos 400. Por eso es muy importante 
saber cu√°l es el l√≠mite f√≠sico que tiene nuestro cl√∫ster. 


Reservar vCPUs para procesos internos
-------------------------------------

El otro punto importante es que no vas a poder aprovechar el 100% de las de vCPU, porque un servidor para funcionar, por ejemplo, 
un Server tiene instalado el sistema operativo, el sistema operativo va a restar algunas vCPUs para que el Server funcione, el 
firewall tambi√©n, los servicios que instales de Getway, Master y Metadata tambi√©n. Entonces, del 100% de las vCPUs que tenga el 
servidor un porcentaje va a estar destinado para que el Server funcione y otro porcentaje va a estar destinado para que t√∫ las 
utilizas en tus procesos. ¬øCu√°les son esos porcentajes? va a depender del rol del servidor. Por ejemplo, en el Gateway, del 100% de 
las vCPUs que est√©n disponibles, un 20% van a estar reservadas para que el servidor funcione y un 80% van a estar reservadas para 
ti. Misma historia con los nodos esclavos, un 20% est√° reservado para que el Server esclavo funcione y el 80% est√° reservado para 
que t√∫ la puedas usar. Para ponerlo en t√©rminos f√°ciles de entender, supongamos que en la potencia computacional completa de tu 
Cl√∫ster tiene 400 vCPUs disponibles, ¬øvamos a tener las 400 vCPUs disponibles? solamente vamos a tener un 80% disponibles de esas 
400 vCPUs. Ahora, ¬øqu√© pasa si no tomas en cuenta esto y piensas que tu l√≠mite real de procesamiento son estas 400 vCPUs? lo que 
va a pasar es que vas a quitarle vCPUs, de las vCPUs que el servidor utiliza para funcionar y probablemente termines colapsando los 
servidores. As√≠ que es muy importante que sepas que solamente tienes un 80% de la capacidad disponible de vCPUs totales, no el 100%.
Si no controlas eso a nivel de infraestructura, porque esto ya lo tienes que configurar con tu herramienta de infraestructura, si 
no tomas en cuenta este factor, es muy probable que los servidores en tu Cl√∫ster comiencen ha colapsar, porque no has puesto un 
l√≠mite en el uso de vCPUs para desarrollos.



--------------------------
RAM
--------------------------


Consideraciones de RAM
-----------------------

Nosotros ya sabemos c√≥mo se procesan nuestros programas, el archivo de datos que queremos procesar se carga a la memoria RAM, si 
por ejemplo, tenemos 100 GB RAM y queremos procesar 1 GB, eso entre f√°cilmente. Se procesa, el procesamiento se hace en las vCPUs. 
Mientras m√°s vCPUs instanciemos, m√°s paralelizable va a ser el proceso. Ya sabemos que estas vCPUs dependiendo de la herramienta 
no van a estar aisladas, si no, que hay containers con las que lo vamos a reservar, por ejemplo, en Hive vimos que son los 
containers Mappers. Para usar 8 vCPUs requerir√≠amos de 4 containers, porque el est√°ndar es que cada container tiene 2 CPUs. Al fin 
y al cabo realmente quien procesa no son los containers, si no, las CPUs dentro de los containers. Procesaremos, haremos lo que 
tengamos que hacer y el proceso tiene un Output, por ejemplo, un output de 10 GB. Eso tambi√©n entra en la memoria RAM, no hay 
problema. 

Pero podr√≠a pasar lo siguiente, entra 10 GB de datos, lo procesamos y el output es de 200 GB, eso ya no entra en la memoria RAM 
(memoria RAM tiene 100 GB), esos 110 GB faltantes, lamentablemente se van a tener que emular de la memoria virtual, as√≠ que va a 
ser muy importante saber cu√°nta memoria RAM vamos a requerir para nuestros procesos a nivel de infraestructura, porque el developer,
podr√≠a decir ‚Äú‚Ä¶oye dame 2 TB de RAM‚Ä¶‚Äù, pero en el Cl√∫ster tenemos el l√≠mite de 1 TB de RAM y obviamente este pedido no va a poder 
ser atendido y vamos a requerir de memoria virtual. As√≠ que a nivel de infraestructura tenemos que tener ya la RAM que van a 
requerir nuestros procesos. 


Reservar RAM para procesos internos
-----------------------------------

Al igual que en el caso de las vCPUs, ac√° tambi√©n existen recomendaciones. Solamente vamos a tener el 80% de la RAM total del 
cl√∫ster para poder utilizarlo en tus procesos. Si en todo el cl√∫ster tienes 1000 GB de memoria RAM, solamente vas a poder utilizar 
800 GB, porque 200 de ellos van a estar reservados para que los servers funcionen. 



--------------------------
DISCO DURO
--------------------------


Consideraciones de Disco duro
-----------------------------


Finalmente, respecto a la parte de Disco duro bueno esto creo que ya es f√°cil de entender para nosotros. Es importante, no 
solamente enfocarnos en la capacidad del Disco duro, si no, en el n√∫mero de los Discos duros, para que el sistema de archivos 
puede paralelizar las escrituras y las lecturas. Adicionalmente a eso, tambi√©n tenemos que saber que existen diferentes tipos de 
Discos duros, por ejemplo, tenemos un Disco duro en donde instalamos el sistema operativo, otro Disco duro dedicado a instalaci√≥n 
de programas, un Disco duro en donde se guardan archivos temporales (tmp), un Disco duro en donde se guardan los logs de los 
servicios (log), el Disco duro asignado a la carpeta ‚Äúhome‚Äù del Getway donde se conectan los usuarios. Estos Discos duros son 
est√°ndar en infraestructura, en el mundo del Big Data solamente vamos a tener que hacer sizing de los Discos duros que almacenan 
datos. ¬øD√≥nde se almacenan los datos? En los nodos esclavos. Todos los nodos, sea cual sea su rol, Getway, Master, Metadata o Slave 
van a requerir de estos Discos duros (Disco Sistema Operativo, Disco Programas, Disco tmp, Disco log, Disco home, Disco de datos),
pero obviamente todos los servidores tienen un sistema operativo instalado, entonces tiene que tener un Disco duro del sistema 
operativo, todos esos son Discos duros est√°ndar de infraestructura. A nivel de Big Data t√∫ solamente te vas a preocupar en hacer 
el sizing de Discos duros para los nodos esclavos, porque ah√≠ es donde se almacena la data. 


---------------------------------------------------------------------------------------------------------------- 
 _____________________________
|                             |
|   Est√°ndar hardware para    |
|   servidores seg√∫n roles    |
|_____________________________|

Para hacer sizing debemos de responder dos preguntas: 

=> ¬øCu√°ntos nodos se necesita seg√∫n el rol? 
=> ¬øCu√°nta RAM, CPU y Disco duro requiere cada servidor seg√∫n su rol? 

Respondamos la pregunta m√°s f√°cil. El est√°ndar hardware seg√∫n el rol. Ya existe un est√°ndar hardware seg√∫n si quieres Getway, 
Master, Metadata o Slave. 


Nodo Gateway
------------

Por ejemplo, esta es la configuraci√≥n est√°ndar m√≠nima que existe para el nodo Gateway: 

32 GB de RAM 
20 vCPUs 

Como en el Gateway no se almacena nada de HDFS, todo eso va en los nodos esclavos, no necesitas discos de datos. Esto es lo m√≠nimo. 

Lo recomendado:

128 GB de RAM 
40 vCPUs 

Ahora, tambi√©n tienes que saber que cada vCPU de un Gateway puede soportar hasta 5 usuarios, por ejemplo, si tuvieses un Getaway 
de 20 vCPUs, ya sabemos, que primero solamente podemos utilizar el 80% de esas vCPUs, eso quiere decir que tendr√≠amos 16 vCPUs a 
nuestra disposici√≥n. Segundo, cada vCPU soporta hasta 5 usuarios y 16 x 5 = 80, eso significa que un Getway con estas 
caracter√≠sticas puede soportar hasta 80 usuarios concurrentes. Tu organizaci√≥n tiene menos de 80 usuarios, en teor√≠a con un Getway 
te podr√≠a bastar, pero todav√≠a no estamos resolviendo el problema de cu√°ntos Gateways necesitamos, solamente estamos diciendo cu√°les 
son las caracter√≠sticas hardware m√≠nimas y recomendadas que debe tener tu Getway para funcionar, ya luego vemos el n√∫mero de 
Gateways necesarios. 


Nodo Master
-----------

En el caso de los Masters, la configuraci√≥n hardware m√≠nima es de:

128 GB de RAM
20 vcPUs 

Pero la recomendada es de: 

256 GB de RAM 
40 vCPUs 

Los nodos Master b√°sicamente reciben peticiones y delegan trabajo, as√≠ que es muy raro que salga de estas recomendaciones. De hecho, 
con el ‚Äúm√≠nimo‚Äù es suficiente. Al menos en la pr√°ctica he visto que 1 nodo Master puede responder bien con eso, ya que, el Master 
solamente es como que pasase la pelota, recibe la petici√≥n y decide en qu√© nodos ejecutar.  


Nodo Slave
-----------

En el caso de los nodos esclavos, aqu√≠ es donde va a estar la potencia de procesamiento. Como m√≠nimo que debemos de tener, el 
est√°ndar nos dice: 

256 GB de RAM 
40 vCPUs
20 TB de Disco duro HDD (Disco duro mec√°nico) para cada nodo esclavo 

Esto es lo mismo. Lo recomendado es:

Lo que el presupuesto de la infraestructura tenga disponible, lo m√°s potente posible que sean los nodos esclavos, porque aqu√≠ es 
donde est√° el procesamiento. 

En la vida real tambi√©n he visto que este m√≠nimo no es respetado y dependiendo del presupuesto que tenga la empresa, puede que a 
veces vayan por la mitad, 128 GB de RAM,  pero no pasa nada, realmente no es eso es lo que se recomienda como m√≠nimo, pero si t√∫ 
necesitas esos servidores m√°s peque√±itos, no pasa nada, los puedes seguir usando, solamente que la explicaci√≥n que es el est√°ndar 
de n√∫mero de nodos Slave va a cambiar un poco. Pero no nos compliquemos, existe un est√°ndar m√≠nimo, pero nada impide que sean de 
menor tama√±o, no es que de pronto si pones un servidor de 128 GB de RAM ya no va a funcionar la paralelizaci√≥n, no, si va a seguir 
funcionando. Como les digo, estos son los n√∫meros que son est√°ndar en la industria, obviamente en una empresa pueden variar.


Nodo Metadata
-------------

Finalmente en el nodo de Metadata como aqui simplemente guardamos, b√°sicamente qu√© tabla es qu√© directorio HDFS, tampoco 
necesitamos tanta potencia. Lo m√≠nimo es: 

32 GB de RAM
10 vCPUs
1 TB de Disco duro SSD para guardar esa metadata 

Si se dan cuenta en la recomendaci√≥n de los nodos esclavos estamos hablando de Discos duros mec√°nicos, mientras que ac√° estamos 
hablando de Discos de almacenamiento en estado s√≥lido. No es necesario que tengas Discos duros SSD para los nodos esclavos, con 
los HDD cl√°sicos es suficiente, incluso aunque vayas a hacer procesamiento de Real time, porque para eso tenemos patrones que van 
a optimizar el uso de estos discos. Si recordamos los patrones, no vas a encontrar diferencia directa si es HDD o SDD. Y 
adicionalmente los discos SSD son m√°s costosos que los HDD, antiguamente el costo era por 10 al menos hace 7 a√±os, ahora que se 
est√°n abaratando los costos de las tecnolog√≠as, el costo de hasta 1 por 3, que significa eso, s√≠ con 100 d√≥lares te compras 1 TB, 
si quisieras un TB de SSD tendr√≠as que hacer una inversi√≥n de 300 d√≥lares, el precio es hasta el triple. Como vas a tener muchos 
nodos esclavos eso implicar√≠a que tu presupuesto en almacenamiento se va a multiplicar mucho, as√≠ que por eso no se recomienda 
nodos esclavos. Para aumentar velocidades se recomienda usar patrones de arquetipo. En  Real time esto va a quedar mas claro. En 
el caso de los nodos de Metadata ah√≠ s√≠ vamos a requerir SSD, porque lo que queremos es cuando HIVE lanza una consulta a la tabla, 
realmente esta viendo que directorio est√° asociado a esa tabla, queremos obtener esa respuesta lo m√°s r√°pido posible para comenzar 
a procesar. Esa informaci√≥n va a estar guardada en SSD, por lo tanto, la lectura de la Metadata va a ser muy r√°pida. Por eso, en 
los nodos Metadata se recomienda como m√≠nimo 1 TB SSD, que por efectos pr√°cticos, ese disco sale 100 dolares, as√≠ que no pasa nada. 


¬øEstos n√∫meros son absolutos? no, son los est√°ndares. Ya dependiendo de la empresa puedes variar este est√°ndar, pero al ser 
est√°ndar vamos a hacer los c√°lculos del n√∫mero de nodos necesarios en funci√≥n de este est√°ndar. Por ejemplo, para el nodo Gateway, 
si tenemos 20 vCPUs, ya sabemos que el 80% ser√≠a 16 vCPUs y como cada vCPU soporta 5 conexiones, tendr√≠amos hasta 80 usuarios 
concurrentes. ¬øQu√© pasa si es que nuestra organizaci√≥n tiene 200 usuarios que van a acceder? como cada servidor Gateway soporta 80 
conexiones y queremos tener hasta 200 conexiones, tendr√≠amos que tener 3 servidores Gateway. Ese c√°lculo lo hemos hecho bas√°ndonos 
en los numeros est√°ndar, si tu organizaci√≥n compra servidores m√°s peque√±os, el c√°lculo va a variar, pero la forma en c√≥mo se 
calcula es la misma. As√≠ que para hacer el c√°lculo del n√∫mero de nodos nos vamos a basar en el est√°ndar. Con eso ya tenemos 
resuelto entonces la primera pregunta ¬øcu√°l es el est√°ndar hardware que necesitamos para cada tipo de nodo? 

---------------------------------------------------------------------------------------------------------------- 
 _____________________________
|                             |
|   Est√°ndar para cantidad    |
|   de nodos seg√∫n rol        |
|_____________________________|

Sizing de nodos por rol
-----------------------

Ahora la siguiente pregunta que tenemos que responder es ¬øcu√°ntos nodos necesitamos, cu√°ntos Getways, Masters, Metadata y Slave? 

Vamos a hacer el sizing de n√∫mero de nodos. Hay una buena noticia, ya tenemos un est√°ndar para los nodos:

+ Gateway
+ Master
+ Metadata

En el caso del Gateway si seguimos las recomendaciones RAM y vCPUs que son est√°ndar, cada nodo recomendado es de 40 vCPUs, cada 
nodo puede soportar hasta 80 conexiones. Supongamos que tienes 160 conexiones simult√°neas necesarias, eso va a implicar que al 
menos requieres dos nodos Gateway. Punto importante, yo solamente tengo 70 conexiones, ¬øeso quiere decir que solo necesito un 
Gateway? no, el est√°ndar de n√∫meros de Gateways como m√≠nimo te dice es que tengas 2, porque puede que colapse uno de estos dos 
nodos por alguna raz√≥n. Si colapsa el que Getway, ya no tienes acceso al Cluster. As√≠ que el est√°ndar es al menos 2 nodos y ya 
sabemos que cada vCPU soporta hasta 5 conexiones simult√°neas, as√≠ que va a depender. Con un nodo tenemos 80 usuarios, con 2 nodos 
tenemos 160 usuarios, con 3 nodos tendr√≠amos 240 conexiones y as√≠ sucesivamente. El n√∫mero de nodos exacto saldr√≠a en funci√≥n de 
las conexiones simult√°neas esperadas al Cl√∫ster. 

¬øQu√© va a pasar en el caso del Master? aqu√≠ ni siquiera vas a tener que pensarlo, el est√°ndar de Industria es 3 Masters, el Master 
simplemente recibe una petici√≥n y delega trabajo. De hecho ni siquiera hace un peque√±o procesamiento, no hace nada. Vamos a ver 
que en el caso de Spark vamos a hacer unos peque√±os procesamientos en el Gateway, pero en el caso del Master, no hace nada. El 
est√°ndar de industria nos dice que al menos tengas 3 Master instalados, por dos razones: porque si colapsa un Master todav√≠a 
tenemos otros dos que van a recibir las peticiones y adem√°s para tener un algoritmo de consenso, ¬øqu√© significa eso? Para ponerlo 
en t√©rminos muy simples, digamos que el Master 1 decidi√≥ que el container se ejecute en el Nodo Slave 1, el Master 2 decidi√≥ que 
el container se ejecute en el Nodo Slave 1 y el Master 3 decidi√≥ que el container se ejecute en el Nodo Slave 2. Hay una mayor 
cantidad de Masters que decidieron que se ejecuten en un mismo servidor, por lo tanto, el container se ejecutar√≠a en ese servidor. 
¬øQu√© pasar√≠a si tuvi√©semos un n√∫mero par de Masters? podr√≠a darse este caso, 2 Masters dicen que se ejecute en el Nodo Slave 1 y 
los otros 2 Masters dicen que se ejecute en Nodo Slave 2, por lo tanto, no se puede tomar una decisi√≥n. Por eso se recomienda tener 
un n√∫mero impar de Masters, pero no pueden ser dos, tienen que ser al menos 3. 

En el caso de los nodos de Metadata el est√°ndar m√≠nimo te dice que tengas dos. ¬øQu√© se almacena en la metadata? que tal tabla es 
tal directorio de HDFS. Si pierdes ese mapeo de metadatos, por m√°s r√©plicas de directorios y archivos que tengas en los esclavos ya 
no sabes qu√© tabla es qu√© directorio, as√≠ que al menos debes tener una copia adicional de esa metadata en tu infraestructura. El 
est√°ndar m√≠nimo es 2, pero ya va a depender de como tu organizaci√≥n trabaje el numero de copias de metadatos. 

---------------------------------------------------------------------------------------------------------------- 
 ________________________
|                        |
|   Casos t√≠picos de     |
|   infraestructura      |
|________________________|

Desarrollo o certificaci√≥n
--------------------------

Ahora nos falta resolver el n√∫mero de nodos esclavos. ¬øCu√°ntos? Obviamente, ya sabemos que aqu√≠ est√° la potencia, va a depender 
del caso. Hay dos casos t√≠picos de infraestructura para los nodos esclavos que se repiten mucho. El de Desarrollo y el de 
Certificaci√≥n. Hay un est√°ndar para ambientes de desarrollo y de certificaci√≥n, el cual te dice ten 10 nodos esclavos. ¬øQu√© es lo 
que pasa en la vida real? tenemos el Cl√∫ster de Desarrollo, el Cl√∫ster de Certificaci√≥n y el Cl√∫ster Productivo. El desarrollador 
se conecta el cl√∫ster de desarrollo, comienza a trabajar, termina de trabajar, saca su proceso de del cluster de desarrollo y lo 
prueba en el ambiente de certificaci√≥n. Si est√° todo correcto, tu proceso ya vive de manera permanente en el cluster Productivo. 
Los clusters de desarrollo y certificaci√≥n se van liberando cada vez que hay un paso a producci√≥n, pero en el cluster productivo 
es donde ya el proceso vive de manera permanente. Al d√≠a siguiente vendr√° desarrollador a otro proceso, lo probar√° y pasar√° al 
cluster productivo y vivir√° de manera permanente en producci√≥n, pero si te das cuenta los clusters de desarrollo y certificaci√≥n 
se van liberando una vez que hay un pase a producci√≥n, as√≠ que no es necesario tener la misma potencia en desarrollo y 
certificaci√≥n que en tu cluster productivo. Hay un est√°ndar que te dice que puedes tener 10 nodos en tu ambiente de desarrollo y 
10 nodos esclavos en tu ambiente productivo y es raro que vayas a requerir m√°s potencia si tienes un flujo de DevOps bien definido 
para que los pases se vayan haciendo entre ambiente, porque implica que ya se van liberando. ¬øCu√°l es el est√°ndar que te dice 
entonces el n√∫mero de nodos esclavos? si est√°s en un cl√∫ster de desarrollo o de certificaci√≥n, el est√°ndar te dice coloca 10 
servidores como m√≠nimo. Obviamente dependiendo del presupuesto puedes tener menos o m√°s servidores, pero esta potencia 
computacional no es la que tu vas a calcular para tu cluster productivo, en donde van a ir acumul√°ndose todos los procesos que 
desarrollemos. 



Producci√≥n
----------

El problema solamente va a estar en el cl√∫ster productivo entonces, ah√≠ s√≠ van a vivir los procesos de manera permanente. 
Inicialmente de alguna manera vamos a tener que hacer el c√°lculo de n√∫meros de servidores esclavos para nuestro entorno productivo. 
Digamos que necesitamos 100 servidores esclavos, a√∫n no sabemos c√≥mo hacerlo, pero digamos que sale este n√∫mero. Esto significa 
que al d√≠a siguiente, van a haber 10000 procesos que vamos a ir desarrollando, eso significa que al d√≠a siguiente de que tu compres 
los 100 servidores, ya tienes implementados estos 10000 procesos, obviamente no. Primero programaras algunos en tu cluster de 
desarrollo, esos algunos se probar√°n en el cluster de certificaci√≥n y si est√°n correctos, reci√©n pasaran al cluster de producci√≥n. 
Quiz√° en el primer mes har√°s 5 procesos, el siguiente mes otros 5 y as√≠. ¬øPor qu√© es importante entender esto? porque si en tu 
c√°lculo de sizing para tus nodos en entorno de producci√≥n te sale 100, no es necesario que los compres inmediatamente, generalmente 
tienes que tener una estrategia de ir tray√©ndolos de poco en poco, por ejemplo, el est√°ndar te dice hay que ir despleg√°ndolos de 
10 en 10, porque al d√≠a siguiente de que sabes la infraestructura, el entorno de producci√≥n no va a tener todos los procesos ya 
implementados, si no, que en funci√≥n de c√≥mo se van implementando ya van a ir viviendo en el cluster productivo, as√≠ que puedes 
tener una estrategia de este tipo. Si te sale este n√∫mero, 100 nodos para dar soporte a un cl√∫ster por 5 a√±os, no significa que 
al d√≠a siguiente todo el presupuesto se vaya para 5 a√±os, puedes tener una estrategia trimestral o semestral de ir comprando los 
servidores en funci√≥n de cu√°ntos procesos vayas desarrollando.

---------------------------------------------------------------------------------------------------------------- 
 _______________________
|                       |
|   Calculo de numero   |
|   de nodos Slave      |
|_______________________|

¬øY c√≥mo saber cu√°ntos servidores esclavos en total vamos a requerir para nuestro entorno productivo? ese es el c√°lculo del n√∫mero 
de nodos esclavos. Y al igual que el tunning, desde el punto de vista acad√©mico, realmente para hacer el c√°lculo de sizing de los 
nodos esclavos, de antemano deber√≠amos saber cu√°ntos procesos van a vivir en el cl√∫ster y cu√°nta RAM y CPU va a requerir cada uno 
de estos procesos para cumplir el nivel de servicio esperado por negocio. Si nosotros pudi√©ramos saber de antemano cu√°nta RAM y 
CPU requiere cada proceso para cumplir ese nivel de servicio, ya sabr√≠amos entonces, pongamos n√∫meros f√°ciles de manejar, cada 
proceso requiere de 5 GB de RAM y 2 vCPUs y tenemos 10000 procesos, por lo tanto, requeriremos de 50000 GB de RAM y 20000 vCPUs y 
listo, como cada server tiene 40 vCPUs, dividimos esto entre el n√∫mero de vCPUs y ya podemos saber el n√∫mero de nodos esclavos. 
Eso ser√≠a lo ideal, pero t√∫ de antemano no puedes saber cu√°nta RAM y CPU van a requerir los procesos que vas a implementar, porque 
se supone que primero preguntas la l√≥gica del negocio y luego implementas el tunning y vas a entender cu√°les son los n√∫meros 
correctos para cumplir ese nivel de servicio esperado. As√≠ que es impr√°ctico, hay mucha teor√≠a acad√©mica que te dice c√≥mo calcular 
el n√∫mero de nodos esclavos en funci√≥n de la RAM y CPU, pero es irreal, porque t√∫ no sabes de antemano cu√°nta RAM y CPU requiere 
cada proceso. As√≠ que vamos a utilizar un m√©todo que no es tan exacto, pero es m√°s pr√°ctico en la vida real. 

Ya sabemos que todo proceso, tenemos muchos procesos, requiere de un input para funcionar. Todos requieren de un input. Si t√∫ a 
nivel de Disco duro como m√≠nimo no puedes almacenar este input no vas a poder procesar nada, tienes que tener el input capturado, 
por ejemplo, con procesos de ETL tienes que capturar esa data input. As√≠ que este va a ser nuestro punto de partida para saber 
cu√°ntos nodos esclavos vamos a requerir, el input de la data que van a utilizar nuestros procesos. Por ejemplo, ¬øqu√© procesos van 
a estar implementados en nuestro Data Lake? Por decir algo, procesos relacionados con la base de datos de clientes, con la base de 
datos de empresas, con la base de datos de transacciones y as√≠, con muchas bases de datos. Perfecto, ya sabemos que el primer paso 
es hacer el CTRL + C y CTRL + V, tenemos que capturar la data. Las tablas de esta base de datos que vamos a cargar, ¬øcu√°nto pesa? 
vamos a poner n√∫meros f√°ciles de manejar, son 3 bases de datos, es decir, las tablas pesan 100 GB, 200 GB y 300 GB respectivamente. 
Supongamos que solamente queremos dichas tablas a nuestro Data Lake. Eso significa que en total los datos hist√≥ricos que queremos 
subir en el Data Lake son 600 GB. Siguiente pregunta, y ¬øhay datos delta (eso quiere decir, por ejemplo, en los archivos de 
transacciones el dia 21 te dejaban un archivo de transacciones, el d√≠a 22 te dejan otro archivo de transacciones)? algunas tablas 
te dir√°n que s√≠ otras tablas quiz√° te digan que no. Para aquellas tablas en donde se diga que s√≠ va a haber un delta que se va a 
dejar cada cierto tiempo, la siguiente pregunta es ¬øcu√°l es la periodicidad, diario, semanal o mensual? Luego, qu√© m√°s tenemos que 
saber, y ¬øcu√°nto pesa ese delta, ese peque√±o archivo de informaci√≥n que vas a dejar?.

                         HISTORICO       DELTA      PERIOCIDAD     PESO ARCHIVO DELTA
 _________              ______________________________________________________________
|         |                         |            |              |                     |
|  MySQL  | --------->     100 GB   |     SI     |    DIARIO    |         1 GB        |  
|_________|                         |            |              |                     |             
 _________                          |            |              |                     |     
|         |                         |            |              |                     | 
|  Oracle | --------->     200 GB   |     NO     |     -----    |         -----       |  
|_________|                         |            |              |                     |         
 _________                          |            |              |                     |     
|         |                         |            |              |                     |  
|   DB2   | --------->     300 GB   |     SI     |    SEMANAL   |         2 GB        |  
|_________|            _____________|____________|______________|_____________________|

Ya tenemos entonces el input qu√© vamos a procesar. ¬øQu√© m√°s tenemos que saber? vamos a estandarizar y calcular el delta anual. Y, 
¬øqu√© es esto del delta anual? para aquellos archivos en donde cada d√≠a te dejan un cachito de la informaci√≥n, multiplicamos el peso 
de esa informaci√≥n por la periodicidad, es decir, 1 GB diario en 1 a√±o ser√≠an 365 GB y 2 GB semanales en 1 a√±o (52 semanas) vamos 
a acumular 104 GB. 

                         HISTORICO       DELTA      PERIOCIDAD     PESO ARCHIVO DELTA      DELTA ANUAL
 _________              __________________________________________________________________________________
|         |                         |            |              |                     |                   |  
|  MySQL  | --------->     100 GB   |     SI     |    DIARIO    |         1 GB        |      365 GB       |  
|_________|                         |            |              |                     |                   |  
 _________                          |            |              |                     |                   |  
|         |                         |            |              |                     |                   |  
|  Oracle | --------->     200 GB   |     NO     |     -----    |         -----       |        0 GB       |   
|_________|                         |            |              |                     |                   |   
 _________                          |            |              |                     |                   |   
|         |                         |            |              |                     |                   |   
|   DB2   | --------->     300 GB   |     SI     |    SEMANAL   |         2 GB        |      104 GB       |   
|_________|            _____________|____________|______________|_____________________|___________________|


Esto significa que la primera carga hist√≥rica de todos los datos nos va a ocupar 600 GB y anualmente vamos a estar cargando 469 GB. 
Ahora, supongamos que el peso hist√≥rico nos sali√≥ 1000 TB y supongamos que el peso del delta por a√±o nos sali√≥ 100 TB. Una vez que 
ya tienes calculado estos 2 numeritos construyes una tabla de este tipo: si quiero tener un soporte de almacenamiento por 1 a√±o 
completo, ¬øcu√°nta capacidad de Disco duro debo de tener? ser√≠a el peso hist√≥rico m√°s el peso anual: 1100 TB. Si quiero tener una 
capacidad de almacenamiento de 2 a√±os, ¬øcu√°nto voy a necesitar? 1000 TB por cu√°ntos a√±os que se generen, ser√≠an 1200 TB. Si queremos 
tener soporte de 3, 4 y 5 a√±os seg√∫n estos n√∫meros, saldr√≠an 1300 TB, 1400 TB y 1500 TB. 

                                                ____________________
                                                        |           |
                                                 1 A√ëO  |  1100 TB  |
                                                        |           |
                                                 2 A√ëO  |  1200 TB  |
                                                        |           |
                                                 3 A√ëO  |  1300 TB  |
                                                        |           |
                                                 4 A√ëO  |  1400 TB  |
                                                        |           |    
                                                 5 A√ëO  |  1500 TB  |
                                                ________|___________|        


Pero recordemos que en sistemas de archivos distribuidos, hay factores de replicaci√≥n, algunos ser√°n por 3, otros ser√°n por 5, pero 
debemos asumir que es por 3, porque de antemano no podemos saber cu√°l va a ser el factor de replicaci√≥n de cada archivo. En tiempo 
real, por ejemplo, algunos van a ser por 5, asume que es por 3. Eso significa que como todo se va a replicar 3 veces, realmente los 
n√∫meros que vas a requerir son estos de aqu√≠:
                                                ________________________________
                                                        |           |           |
                                                 1 A√ëO  |  1100 TB  |  3300 TB  |
                                                        |           |           |
                                                 2 A√ëO  |  1200 TB  |  3600 TB  |
                                                        |           |           |
                                                 3 A√ëO  |  1300 TB  |  3900 TB  |
                                                        |           |           |
                                                 4 A√ëO  |  1400 TB  |  4200 TB  |
                                                        |           |           |
                                                 5 A√ëO  |  1500 TB  |  4500 TB  |
                                                ________|___________|___________|

Finalmente, el tercer paso. Seg√∫n el est√°ndar de infraestructura como m√≠nimo tenemos Discos duros de 20 TB cada uno y el recomendado 
es de 100 TB. Vamos a trabajar con el recomendado. El Cluster necesita de 3300 TB, cada servidor esclavo almacena 100 TB, ¬øcu√°ntos 
servidores esclavos vamos a requerir para almacenar 3300 TB? 33 servers. Si queremos tener un soporte de 3600 TB, ser√≠an 36 servers, 
si queremos tener un soporte para 3900 TB serian 39 servers, para 4200 TB serian 42 servers y para 4500 TB serian 45 servers. Para 
que nuestro cl√∫ster pueda funcionar bien por 1 a√±o, 33 servidores, por 2 a√±os 36 servidores, por 3 a√±os 39 servidores y as√≠ 
sucesivamente. 
                                        _________________________________________________
                                                |           |           |                |   
                                         1 A√ëO  |  1100 TB  |  3300 TB  |  33 servidores |
                                                |           |           |                | 
                                         2 A√ëO  |  1200 TB  |  3600 TB  |  36 servidores |                
                                                |           |           |                | 
                                         3 A√ëO  |  1300 TB  |  3900 TB  |  39 servidores |
                                                |           |           |                | 
                                         4 A√ëO  |  1400 TB  |  4200 TB  |  42 servidores |
                                                |           |           |                | 
                                         5 A√ëO  |  1500 TB  |  4500 TB  |  45 servidores |
                                        ________|___________|___________|________________|

Finalmente, lo √∫ltimo que queda por hacer son los costos. Suponiendo que tenemos servidores de 100 TB y seguimos las recomendaciones, 
256 GB de RAM y 40 vCPUs, vamos a trabajar con el est√°ndar para hacer el c√°lculo, probablemente esto nos salga:

10000 USD por servidor. Si cada servidor est√° a 10000 USD y tenemos 33 servidores, eso significa que el presupuesto para 1 a√±o ser√≠a 
de 330000 USD, 360000 USD,  390000 USD y as√≠ sucesivamente. Obviamente va depender, o sea, si tu empresa tiene 3300 TB ser√≠a una 
empresa grande y estos costos ser√≠an f√°cil, por ejemplo, para un banco estos n√∫meros en d√≥lares no es nada, va a depender por eso de 
lo que quieras hacer con el servidor. Listo, ya tienes entonces esta informaci√≥n que vas a presentar. Al tomador de decisiones que 
va a aprobar el presupuesto le va a importar lo siguiente: 

1.- El tiempo en el cual no vamos a requerir (en a√±os) m√°s servidores 
2.- La capacidad que en total vamos a tener 3300 TB y as√≠ sucesivamente
3.- El n√∫mero de servidores para soportar esa capacidad
4.- Y lo m√°s importante, el costo. 
                                        ______________________________________________________
                                                |           |                |                |   
                                         1 A√ëO  |  3300 TB  |  33 servidores |  330.000 USD   |
                                                |           |                |                | 
                                         2 A√ëO  |  3600 TB  |  36 servidores |  360.000 USD   |                
                                                |           |                |                | 
                                         3 A√ëO  |  3900 TB  |  39 servidores |  390.000 USD   |
                                                |           |                |                | 
                                         4 A√ëO  |  4200 TB  |  42 servidores |  420.000 USD   |
                                                |           |                |                | 
                                         5 A√ëO  |  4500 TB  |  45 servidores |  450.000 USD   |
                                        ________|___________|________________|________________|


¬øQu√© es lo que podr√≠a pasar? tenemos m√∫ltiples opciones elige la que tu desees. Mira, en 5 a√±os va a salir muy caro, voy a elegir 
tener un Cluster que no requiera m√°s soporte por 2 a√±os, el cual requiere de 36 servidores. Yo como encargado de la parte de 
infraestructura, digo "...perfecto, recuerda que para amortizar la inversi√≥n no vamos a requerir tener los 36 servidores al d√≠a 
siguiente, podemos seguir una estrategia trimestral de ir tray√©ndolos, en el primer trimestre 10, en el segundo 10, en el tercer 
trimestre 10 y finalmente en √∫ltimo trimestre 6 servidores...". De esa manera en el trimestre uno cuando muestras tu reporte de 
presupuestos de gerencia, no va a haber un pico de dinero que se ha gastado, si no, que va a estar estable el presupuesto, cada 
trimestre se est√° gastando una parte del presupuesto. Podr√≠a pasar eso. Lo otro que podr√≠a pasar es que te podr√≠an decir: 
"...mira yo quiero tener soporte para 1 a√±o pero 330000 USD es demasiado, puedo gastar la tercera parte, 100000 USD..." perfecto, 
no pasa nada, si tenemos servidores de 300 GB de RAM y 30 vCPUs, pero t√∫ no tienes presupuesto para comprar estos, el Disco duro 
no se puede mover, pero s√≠ podemos sacrificar potencia computacional, entonces compremos servidores de 100 GB de RAM y de 10 vCPUs, 
ya que, no hay presupuesto para eso. Seguiremos teniendo el mismo n√∫mero de servidores, pero ya ser√°n menos potentes. ¬øCu√°l es el 
punto importante aqu√≠? ya tienes n√∫meros y un est√°ndar con el cual puede ser tu punto de partida para empezar a ver c√≥mo calcular 
el presupuesto de infraestructura y llegar a un consenso con el encargado de presupuesto y a plantear estrategias de despliegue 
para que el presupuesto no se gaste tampoco de un d√≠a para otro.

En resumen, el punto de partida c√°lculo son los pesos de las fuentes que vamos a cargar al Data Lake y construir una tabla para 
mostrar al tomadro de decisiones y decirle: ‚Äú‚Ä¶ mira con tantos a√±os, tenemos tanto peso y vamos a tener tantos servidores y eso 
nos sale tanto en $...‚Äù y en funci√≥n de eso comenzar a negociar el presupuesto.

Listo, se tom√≥ la decisi√≥n. Ahora nos queda por realizar la instalaci√≥n del Cluster. Un cl√∫ster uni-nodal que en la vida real se 
utiliza para que uno practique y el verdadero que es el cl√∫ster multi-nodal. Si quieres practicar con la instalaci√≥n, se recomienda 
que primero se instale el cl√∫ster uni-nodal y luego vayas al multi-nodal. Pero te aseguro que dentro de una empresa es m√°s valorado 
saber hacer un buen sizing que saber instalar el Cl√∫ster, porque en el sizing va a salir al final el presupuesto y ah√≠ es donde se 
va a justificar la inversi√≥n, la infraestructura generalmente es la parte m√°s costosa, al menos al inicio en el despliegue de un 
Cluster.

El √∫ltimo punto importante que hay que entender es que el sizing es independiente de si esto va a ir en on-premise o en Cloud. Al 
final, si se necesitan 10 servidores, siempre van a ser 10 servidores, sea en Azure, en Amazon, sea en nuestras propias oficinas. 
En el video de instalaci√≥n del Cluster, esta se hace sobre servidores de Azure, pero el mismo video puede ser aplicado tanto para 
servidores de Amazon o servidores de Huawei de cualquier otro sistema Cloud, ya que, los videos lo hemos hecho agn√≥sticos al 
proveedor, para que puedas practicar en tu proveedor favorito. De hecho cada proveedor te vende Cluster pre-montados, desde el 
punto de vista si tu los montaras manualmente te va a salir m√°s caro. As√≠ que se recomienda un tipo de instalaci√≥n m√°s a medida.